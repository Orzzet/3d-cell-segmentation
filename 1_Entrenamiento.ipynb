{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#!{sys.executable} -m pip install torchio\n",
    "#!{sys.executable} -m pip install scikit-image\n",
    "#!{sys.executable} -m pip install kornia\n",
    "#!{sys.executable} -m pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "MODEL_NAME = 'test2'\n",
    "FILENAME = MODEL_NAME + '.pth'\n",
    "DIM_SIZE_REDUCTION = (1,1,1)\n",
    "MODE = 'target'\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from utils import compare_output, metrics, draw_images, plot_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CellsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Credits instructions: https://torchio.readthedocs.io/#credits\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torchio.transforms as transformsio\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import kornia.augmentation as K\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataset import PATHS\n",
    "\n",
    "train_path = PATHS['LQ_TRAIN']\n",
    "valid_path = PATHS['LQ_VALID']\n",
    "test_path = PATHS['LQ_TEST']\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 1\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([transformsio.ZNormalization()])\n",
    "transform_augmentation = nn.Sequential(\n",
    "    K.RandomDepthicalFlip3D(same_on_batch=True), \n",
    "    K.RandomHorizontalFlip3D(same_on_batch=True), \n",
    "    K.RandomVerticalFlip3D(same_on_batch=True),\n",
    "#    K.RandomRotation3D((0.5, 3, 3), same_on_batch=True)\n",
    ")\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = CellsDataset(train_path, target_mode=MODE, transform=transform, transform_augmentation=transform_augmentation, dim_size_reduction=DIM_SIZE_REDUCTION)\n",
    "valid_data = CellsDataset(valid_path, target_mode=MODE, transform=transform, dim_size_reduction=DIM_SIZE_REDUCTION)\n",
    "test_data = CellsDataset(test_path, target_mode=MODE, transform=transform, dim_size_reduction=DIM_SIZE_REDUCTION)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura U-Net reducida. Pensada para hacer entrenamientos rápidos para hacer pruebas en las que la calidad del resultado no importe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MiniUNet3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura U-Net completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import UNet3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciación del modelo. Si se usa CUDA esperar unos segundos a que el modelo se cargue en GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet3D(1,2)\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizador y definición de funciones de pérdida. También se usa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#from apex import amp\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=0.00001)\n",
    "\n",
    "# https://github.com/mcarilli/mixed_precision_references/blob/master/Pytorch_Devcon_2019/devcon_2019_mcarilli_final.pdf\n",
    "\n",
    "#model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "def target_to_one_hot(target):\n",
    "    temp = torch.reshape(target, (-1,)).long()\n",
    "    target = torch.zeros([torch.numel(temp), 2])\n",
    "    target[torch.arange(torch.numel(temp)),temp] = 1\n",
    "    return target\n",
    "\n",
    "from losses import simple_dice_loss3D, WeightedCrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Tiempo:37s \tTraining Loss: 0.794092 \tValidation Loss: 0.797513\n",
      "Validation loss decreased. Train loss: 0.794092 Validation Loss: (inf --> 0.797513).  Saving model ...\n",
      "Epoch: 2 Tiempo:37s \tTraining Loss: 0.789674 \tValidation Loss: 0.797513\n",
      "Validation loss decreased. Train loss: 0.789674 Validation Loss: (0.797513 --> 0.797513).  Saving model ...\n",
      "Epoch: 3 Tiempo:37s \tTraining Loss: 0.789674 \tValidation Loss: 0.797513\n",
      "Validation loss decreased. Train loss: 0.789674 Validation Loss: (0.797513 --> 0.797513).  Saving model ...\n",
      "Epoch: 4 Tiempo:37s \tTraining Loss: 0.789674 \tValidation Loss: 0.797513\n",
      "Validation loss decreased. Train loss: 0.789674 Validation Loss: (0.797513 --> 0.797513).  Saving model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bdec972c2232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# update training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdatasize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# número de epochs para entrenar el modelo\n",
    "n_epochs = 400\n",
    "\n",
    "# wce, dice\n",
    "LOSS_FUNCTION = 'dice'\n",
    "SAVE_MODEL = True\n",
    "\n",
    "if LOSS_FUNCTION == 'wce':\n",
    "    criterion = WeightedCrossEntropyLoss()\n",
    "\n",
    "exists_best_model = False\n",
    "if os.path.isfile(FILENAME):\n",
    "    checkpoint = torch.load(FILENAME)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    valid_losses = checkpoint['valid_losses']\n",
    "    current_epoch = checkpoint['epochs']\n",
    "    best_model_state_dict = checkpoint['best_model_state_dict']\n",
    "    best_optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    #amp.load_state_dict(checkpoint['amp_state_dict'])\n",
    "    exists_best_model = True\n",
    "else:\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    current_epoch = 0\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "start_training = time.time()\n",
    "for epoch in range(current_epoch+1, current_epoch + n_epochs + 1):\n",
    "    start_epoch = time.time()\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target, correct_cell_count, resized_cell_count in train_loader:\n",
    "        target = target.squeeze(0)\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data = Variable(data).cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        datasize = data.size(0)\n",
    "        del data\n",
    "        if LOSS_FUNCTION == 'dice':\n",
    "            target = target_to_one_hot(target).float()\n",
    "            if train_on_gpu:\n",
    "                target = Variable(target).cuda()\n",
    "            # calculate the batch loss\n",
    "            criterion1 = nn.Softmax(dim=1)\n",
    "            output = output.permute(0,2,3,4,1).contiguous().view(-1,2).float()\n",
    "            loss = simple_dice_loss3D(criterion1(output), target)\n",
    "        elif LOSS_FUNCTION == 'wce':\n",
    "            if train_on_gpu:\n",
    "                target = Variable(target).cuda().long()\n",
    "            loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        #with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        #    scaled_loss.backward()\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item() * datasize\n",
    "        del target\n",
    "        del output\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target, correct_cell_count, resized_cell_count in valid_loader:\n",
    "        target = target.squeeze(0)\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data = Variable(data).cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        datasize = data.size(0)\n",
    "        del data\n",
    "        if LOSS_FUNCTION == 'dice':\n",
    "            target = target_to_one_hot(target).float()\n",
    "            if train_on_gpu:\n",
    "                target = Variable(target).cuda()\n",
    "            # calculate the batch loss\n",
    "            criterion1 = nn.Softmax(dim=1)\n",
    "            output = output.permute(0,2,3,4,1).contiguous().view(-1,2).float()\n",
    "            loss = simple_dice_loss3D(criterion1(output), target)\n",
    "        elif LOSS_FUNCTION == 'wce':\n",
    "            if train_on_gpu:\n",
    "                target = Variable(target).cuda().long()\n",
    "            loss = criterion(output, target)\n",
    "        del target\n",
    "        del output\n",
    "        # update average validation loss\n",
    "        valid_loss += loss.item() * datasize\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    # print training/validation statistics\n",
    "    print('Epoch: {} Tiempo:{:.0f}s \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, time.time()-start_epoch, train_loss, valid_loss))\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        best_optimizer_state_dict = optimizer.state_dict()\n",
    "        print('Validation loss decreased. Train loss: {:.6f} Validation Loss: ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        train_loss,\n",
    "        valid_loss_min,\n",
    "        valid_loss,\n",
    "        ))\n",
    "        valid_loss_min = valid_loss\n",
    "        exists_best_model = True\n",
    "    if exists_best_model:\n",
    "        torch.save({\n",
    "            'epochs': epoch,\n",
    "            'best_model_state_dict': best_model_state_dict,\n",
    "            'best_optimizer_state_dict' : best_optimizer_state_dict,\n",
    "            'model_state_dict' : model.state_dict(),\n",
    "            'optimizer_state_dict' : optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'valid_losses': valid_losses,\n",
    "            'valid_loss_min': valid_loss_min,\n",
    "            #'amp_state_dict': amp.state_dict()\n",
    "        }, FILENAME)\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "plot_epochs(train_losses, valid_losses, MODEL_NAME)\n",
    "metrics(model, test_data, save=True, model_name=MODEL_NAME)\n",
    "\n",
    "print(\"Entrenamiento terminado en {:.2f}m\".format((time.time() - start_training)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
