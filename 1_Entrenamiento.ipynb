{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#!{sys.executable} -m pip install torchio\n",
    "#!{sys.executable} -m pip install scikit-image\n",
    "#!{sys.executable} -m pip install kornia\n",
    "#!{sys.executable} -m pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "MODEL_NAME = 'unet6t'\n",
    "FILENAME = MODEL_NAME + '.pth'\n",
    "DIM_SIZE_REDUCTION = (1,1,1)\n",
    "MODE = 'target'\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from utils import compare_output, metrics, draw_images, plot_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CellsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Credits instructions: https://torchio.readthedocs.io/#credits\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torchio.transforms as transformsio\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import kornia.augmentation as K\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataset import PATHS\n",
    "\n",
    "train_path = PATHS['LQ_TRAIN']\n",
    "valid_path = PATHS['LQ_VALID']\n",
    "test_path = PATHS['LQ_TEST']\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 1\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([transformsio.ZNormalization()])\n",
    "transform_augmentation = nn.Sequential(\n",
    "    K.RandomDepthicalFlip3D(same_on_batch=True), \n",
    "    K.RandomHorizontalFlip3D(same_on_batch=True), \n",
    "    K.RandomVerticalFlip3D(same_on_batch=True),\n",
    "#    K.RandomRotation3D((0.5, 3, 3), same_on_batch=True)\n",
    ")\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = CellsDataset(train_path, target_mode=MODE, transform=transform, transform_augmentation=transform_augmentation, dim_size_reduction=DIM_SIZE_REDUCTION)\n",
    "valid_data = CellsDataset(valid_path, target_mode=MODE, transform=transform, dim_size_reduction=DIM_SIZE_REDUCTION)\n",
    "test_data = CellsDataset(test_path, target_mode=MODE, transform=transform, dim_size_reduction=DIM_SIZE_REDUCTION)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura U-Net reducida. Pensada para hacer entrenamientos rápidos para hacer pruebas en las que la calidad del resultado no importe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MiniUNet3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura U-Net completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import UNet3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciación del modelo. Si se usa CUDA esperar unos segundos a que el modelo se cargue en GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet3D(1,2)\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizador y definición de funciones de pérdida. También se usa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#from apex import amp\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=0.00001)\n",
    "\n",
    "# https://github.com/mcarilli/mixed_precision_references/blob/master/Pytorch_Devcon_2019/devcon_2019_mcarilli_final.pdf\n",
    "\n",
    "#model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "def target_to_one_hot(target):\n",
    "    temp = torch.reshape(target, (-1,)).long()\n",
    "    target = torch.zeros([torch.numel(temp), 2])\n",
    "    target[torch.arange(torch.numel(temp)),temp] = 1\n",
    "    return target\n",
    "\n",
    "from losses import simple_dice_loss3D, WeightedCrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Tiempo:38s \tTraining Loss: 0.753476 \tValidation Loss: 0.718838\n",
      "Validation loss decreased. Train loss: 0.753476 Validation Loss: (inf --> 0.718838).  Saving model ...\n",
      "Epoch: 2 Tiempo:38s \tTraining Loss: 0.700408 \tValidation Loss: 0.719370\n",
      "Epoch: 3 Tiempo:37s \tTraining Loss: 0.685912 \tValidation Loss: 0.701134\n",
      "Validation loss decreased. Train loss: 0.685912 Validation Loss: (0.718838 --> 0.701134).  Saving model ...\n",
      "Epoch: 4 Tiempo:38s \tTraining Loss: 0.682838 \tValidation Loss: 0.705787\n",
      "Epoch: 5 Tiempo:38s \tTraining Loss: 0.677547 \tValidation Loss: 0.697075\n",
      "Validation loss decreased. Train loss: 0.677547 Validation Loss: (0.701134 --> 0.697075).  Saving model ...\n",
      "Epoch: 6 Tiempo:37s \tTraining Loss: 0.676715 \tValidation Loss: 0.695158\n",
      "Validation loss decreased. Train loss: 0.676715 Validation Loss: (0.697075 --> 0.695158).  Saving model ...\n",
      "Epoch: 7 Tiempo:38s \tTraining Loss: 0.674884 \tValidation Loss: 0.692564\n",
      "Validation loss decreased. Train loss: 0.674884 Validation Loss: (0.695158 --> 0.692564).  Saving model ...\n",
      "Epoch: 8 Tiempo:38s \tTraining Loss: 0.672744 \tValidation Loss: 0.689594\n",
      "Validation loss decreased. Train loss: 0.672744 Validation Loss: (0.692564 --> 0.689594).  Saving model ...\n",
      "Epoch: 9 Tiempo:37s \tTraining Loss: 0.672283 \tValidation Loss: 0.695390\n",
      "Epoch: 10 Tiempo:38s \tTraining Loss: 0.672530 \tValidation Loss: 0.686797\n",
      "Validation loss decreased. Train loss: 0.672530 Validation Loss: (0.689594 --> 0.686797).  Saving model ...\n",
      "Epoch: 11 Tiempo:37s \tTraining Loss: 0.672066 \tValidation Loss: 0.686605\n",
      "Validation loss decreased. Train loss: 0.672066 Validation Loss: (0.686797 --> 0.686605).  Saving model ...\n",
      "Epoch: 12 Tiempo:37s \tTraining Loss: 0.390606 \tValidation Loss: 0.256311\n",
      "Validation loss decreased. Train loss: 0.390606 Validation Loss: (0.686605 --> 0.256311).  Saving model ...\n",
      "Epoch: 13 Tiempo:37s \tTraining Loss: 0.227736 \tValidation Loss: 0.229733\n",
      "Validation loss decreased. Train loss: 0.227736 Validation Loss: (0.256311 --> 0.229733).  Saving model ...\n",
      "Epoch: 14 Tiempo:37s \tTraining Loss: 0.205448 \tValidation Loss: 0.226305\n",
      "Validation loss decreased. Train loss: 0.205448 Validation Loss: (0.229733 --> 0.226305).  Saving model ...\n",
      "Epoch: 15 Tiempo:38s \tTraining Loss: 0.200022 \tValidation Loss: 0.244216\n",
      "Epoch: 16 Tiempo:38s \tTraining Loss: 0.182113 \tValidation Loss: 0.219712\n",
      "Validation loss decreased. Train loss: 0.182113 Validation Loss: (0.226305 --> 0.219712).  Saving model ...\n",
      "Epoch: 17 Tiempo:37s \tTraining Loss: 0.177401 \tValidation Loss: 0.210163\n",
      "Validation loss decreased. Train loss: 0.177401 Validation Loss: (0.219712 --> 0.210163).  Saving model ...\n",
      "Epoch: 18 Tiempo:37s \tTraining Loss: 0.177521 \tValidation Loss: 0.216715\n",
      "Epoch: 19 Tiempo:37s \tTraining Loss: 0.166569 \tValidation Loss: 0.195425\n",
      "Validation loss decreased. Train loss: 0.166569 Validation Loss: (0.210163 --> 0.195425).  Saving model ...\n",
      "Epoch: 20 Tiempo:37s \tTraining Loss: 0.166579 \tValidation Loss: 0.215961\n",
      "Epoch: 21 Tiempo:38s \tTraining Loss: 0.160906 \tValidation Loss: 0.209765\n",
      "Epoch: 22 Tiempo:37s \tTraining Loss: 0.163492 \tValidation Loss: 0.200723\n",
      "Epoch: 23 Tiempo:37s \tTraining Loss: 0.154681 \tValidation Loss: 0.190484\n",
      "Validation loss decreased. Train loss: 0.154681 Validation Loss: (0.195425 --> 0.190484).  Saving model ...\n",
      "Epoch: 24 Tiempo:37s \tTraining Loss: 0.153315 \tValidation Loss: 0.203929\n",
      "Epoch: 25 Tiempo:37s \tTraining Loss: 0.156834 \tValidation Loss: 0.183975\n",
      "Validation loss decreased. Train loss: 0.156834 Validation Loss: (0.190484 --> 0.183975).  Saving model ...\n",
      "Epoch: 26 Tiempo:38s \tTraining Loss: 0.159083 \tValidation Loss: 0.206308\n",
      "Epoch: 27 Tiempo:37s \tTraining Loss: 0.154417 \tValidation Loss: 0.189816\n",
      "Epoch: 28 Tiempo:38s \tTraining Loss: 0.145305 \tValidation Loss: 0.199210\n",
      "Epoch: 29 Tiempo:37s \tTraining Loss: 0.146447 \tValidation Loss: 0.197057\n",
      "Epoch: 30 Tiempo:37s \tTraining Loss: 0.145273 \tValidation Loss: 0.179405\n",
      "Validation loss decreased. Train loss: 0.145273 Validation Loss: (0.183975 --> 0.179405).  Saving model ...\n",
      "Epoch: 31 Tiempo:38s \tTraining Loss: 0.153632 \tValidation Loss: 0.212219\n",
      "Epoch: 32 Tiempo:37s \tTraining Loss: 0.147097 \tValidation Loss: 0.184100\n",
      "Epoch: 33 Tiempo:37s \tTraining Loss: 0.142444 \tValidation Loss: 0.181915\n",
      "Epoch: 34 Tiempo:37s \tTraining Loss: 0.141289 \tValidation Loss: 0.189807\n",
      "Epoch: 35 Tiempo:37s \tTraining Loss: 0.138863 \tValidation Loss: 0.187009\n",
      "Epoch: 36 Tiempo:37s \tTraining Loss: 0.139510 \tValidation Loss: 0.177674\n",
      "Validation loss decreased. Train loss: 0.139510 Validation Loss: (0.179405 --> 0.177674).  Saving model ...\n",
      "Epoch: 37 Tiempo:38s \tTraining Loss: 0.143069 \tValidation Loss: 0.185299\n",
      "Epoch: 38 Tiempo:38s \tTraining Loss: 0.141945 \tValidation Loss: 0.183153\n",
      "Epoch: 39 Tiempo:38s \tTraining Loss: 0.138438 \tValidation Loss: 0.180657\n",
      "Epoch: 40 Tiempo:38s \tTraining Loss: 0.133664 \tValidation Loss: 0.194159\n",
      "Epoch: 41 Tiempo:38s \tTraining Loss: 0.134398 \tValidation Loss: 0.175495\n",
      "Validation loss decreased. Train loss: 0.134398 Validation Loss: (0.177674 --> 0.175495).  Saving model ...\n",
      "Epoch: 42 Tiempo:38s \tTraining Loss: 0.135521 \tValidation Loss: 0.188609\n",
      "Epoch: 43 Tiempo:37s \tTraining Loss: 0.131618 \tValidation Loss: 0.187053\n",
      "Epoch: 44 Tiempo:38s \tTraining Loss: 0.132252 \tValidation Loss: 0.183616\n",
      "Epoch: 45 Tiempo:38s \tTraining Loss: 0.133125 \tValidation Loss: 0.173741\n",
      "Validation loss decreased. Train loss: 0.133125 Validation Loss: (0.175495 --> 0.173741).  Saving model ...\n",
      "Epoch: 46 Tiempo:38s \tTraining Loss: 0.131646 \tValidation Loss: 0.187690\n",
      "Epoch: 47 Tiempo:37s \tTraining Loss: 0.136684 \tValidation Loss: 0.178464\n",
      "Epoch: 48 Tiempo:38s \tTraining Loss: 0.133947 \tValidation Loss: 0.176031\n",
      "Epoch: 49 Tiempo:38s \tTraining Loss: 0.132103 \tValidation Loss: 0.181302\n",
      "Epoch: 50 Tiempo:37s \tTraining Loss: 0.127991 \tValidation Loss: 0.174397\n",
      "Epoch: 51 Tiempo:38s \tTraining Loss: 0.126479 \tValidation Loss: 0.179332\n",
      "Epoch: 52 Tiempo:38s \tTraining Loss: 0.129474 \tValidation Loss: 0.167345\n",
      "Validation loss decreased. Train loss: 0.129474 Validation Loss: (0.173741 --> 0.167345).  Saving model ...\n",
      "Epoch: 53 Tiempo:37s \tTraining Loss: 0.129834 \tValidation Loss: 0.171705\n",
      "Epoch: 54 Tiempo:37s \tTraining Loss: 0.128976 \tValidation Loss: 0.187678\n",
      "Epoch: 55 Tiempo:38s \tTraining Loss: 0.133211 \tValidation Loss: 0.168778\n",
      "Epoch: 56 Tiempo:37s \tTraining Loss: 0.134268 \tValidation Loss: 0.173270\n",
      "Epoch: 57 Tiempo:38s \tTraining Loss: 0.132731 \tValidation Loss: 0.186931\n",
      "Epoch: 58 Tiempo:37s \tTraining Loss: 0.133432 \tValidation Loss: 0.177816\n",
      "Epoch: 59 Tiempo:38s \tTraining Loss: 0.139943 \tValidation Loss: 0.198155\n",
      "Epoch: 60 Tiempo:37s \tTraining Loss: 0.132668 \tValidation Loss: 0.181160\n",
      "Epoch: 61 Tiempo:37s \tTraining Loss: 0.133566 \tValidation Loss: 0.179637\n",
      "Epoch: 62 Tiempo:37s \tTraining Loss: 0.132160 \tValidation Loss: 0.172159\n",
      "Epoch: 63 Tiempo:38s \tTraining Loss: 0.134075 \tValidation Loss: 0.177572\n",
      "Epoch: 64 Tiempo:38s \tTraining Loss: 0.132936 \tValidation Loss: 0.181903\n",
      "Epoch: 65 Tiempo:38s \tTraining Loss: 0.132058 \tValidation Loss: 0.167807\n",
      "Epoch: 66 Tiempo:38s \tTraining Loss: 0.128473 \tValidation Loss: 0.180328\n",
      "Epoch: 67 Tiempo:38s \tTraining Loss: 0.127961 \tValidation Loss: 0.172333\n",
      "Epoch: 68 Tiempo:38s \tTraining Loss: 0.127672 \tValidation Loss: 0.186906\n",
      "Epoch: 69 Tiempo:38s \tTraining Loss: 0.131516 \tValidation Loss: 0.175769\n",
      "Epoch: 70 Tiempo:37s \tTraining Loss: 0.123840 \tValidation Loss: 0.175366\n",
      "Epoch: 71 Tiempo:38s \tTraining Loss: 0.129026 \tValidation Loss: 0.170668\n",
      "Epoch: 72 Tiempo:37s \tTraining Loss: 0.124670 \tValidation Loss: 0.167126\n",
      "Validation loss decreased. Train loss: 0.124670 Validation Loss: (0.167345 --> 0.167126).  Saving model ...\n",
      "Epoch: 73 Tiempo:38s \tTraining Loss: 0.133061 \tValidation Loss: 0.173558\n",
      "Epoch: 74 Tiempo:38s \tTraining Loss: 0.132761 \tValidation Loss: 0.180577\n",
      "Epoch: 75 Tiempo:38s \tTraining Loss: 0.128425 \tValidation Loss: 0.170190\n",
      "Epoch: 76 Tiempo:38s \tTraining Loss: 0.126106 \tValidation Loss: 0.181291\n",
      "Epoch: 77 Tiempo:38s \tTraining Loss: 0.128670 \tValidation Loss: 0.176251\n",
      "Epoch: 78 Tiempo:37s \tTraining Loss: 0.124310 \tValidation Loss: 0.175038\n",
      "Epoch: 79 Tiempo:38s \tTraining Loss: 0.124756 \tValidation Loss: 0.170773\n",
      "Epoch: 80 Tiempo:38s \tTraining Loss: 0.122945 \tValidation Loss: 0.174940\n",
      "Epoch: 81 Tiempo:38s \tTraining Loss: 0.129682 \tValidation Loss: 0.177689\n",
      "Epoch: 82 Tiempo:37s \tTraining Loss: 0.125335 \tValidation Loss: 0.172711\n",
      "Epoch: 83 Tiempo:38s \tTraining Loss: 0.128709 \tValidation Loss: 0.175767\n",
      "Epoch: 84 Tiempo:38s \tTraining Loss: 0.130024 \tValidation Loss: 0.174863\n",
      "Epoch: 85 Tiempo:38s \tTraining Loss: 0.127140 \tValidation Loss: 0.169315\n",
      "Epoch: 86 Tiempo:37s \tTraining Loss: 0.133549 \tValidation Loss: 0.178266\n",
      "Epoch: 87 Tiempo:38s \tTraining Loss: 0.129465 \tValidation Loss: 0.174995\n",
      "Epoch: 88 Tiempo:38s \tTraining Loss: 0.153378 \tValidation Loss: 0.202216\n",
      "Epoch: 89 Tiempo:38s \tTraining Loss: 0.132105 \tValidation Loss: 0.173471\n",
      "Epoch: 90 Tiempo:38s \tTraining Loss: 0.129777 \tValidation Loss: 0.199812\n",
      "Epoch: 91 Tiempo:38s \tTraining Loss: 0.139187 \tValidation Loss: 0.174500\n",
      "Epoch: 92 Tiempo:38s \tTraining Loss: 0.135586 \tValidation Loss: 0.173393\n",
      "Epoch: 93 Tiempo:38s \tTraining Loss: 0.132894 \tValidation Loss: 0.185906\n",
      "Epoch: 94 Tiempo:38s \tTraining Loss: 0.128849 \tValidation Loss: 0.174328\n",
      "Epoch: 95 Tiempo:37s \tTraining Loss: 0.131319 \tValidation Loss: 0.173079\n",
      "Epoch: 96 Tiempo:38s \tTraining Loss: 0.127060 \tValidation Loss: 0.174490\n",
      "Epoch: 97 Tiempo:38s \tTraining Loss: 0.126050 \tValidation Loss: 0.178662\n",
      "Epoch: 98 Tiempo:38s \tTraining Loss: 0.126014 \tValidation Loss: 0.180770\n",
      "Epoch: 99 Tiempo:37s \tTraining Loss: 0.121024 \tValidation Loss: 0.165957\n",
      "Validation loss decreased. Train loss: 0.121024 Validation Loss: (0.167126 --> 0.165957).  Saving model ...\n",
      "Epoch: 100 Tiempo:38s \tTraining Loss: 0.126485 \tValidation Loss: 0.172655\n",
      "Epoch: 101 Tiempo:38s \tTraining Loss: 0.126188 \tValidation Loss: 0.174519\n",
      "Epoch: 102 Tiempo:38s \tTraining Loss: 0.119049 \tValidation Loss: 0.168074\n",
      "Epoch: 103 Tiempo:38s \tTraining Loss: 0.118658 \tValidation Loss: 0.168976\n",
      "Epoch: 104 Tiempo:38s \tTraining Loss: 0.122292 \tValidation Loss: 0.186262\n",
      "Epoch: 105 Tiempo:38s \tTraining Loss: 0.122320 \tValidation Loss: 0.169553\n",
      "Epoch: 106 Tiempo:38s \tTraining Loss: 0.122423 \tValidation Loss: 0.168418\n",
      "Epoch: 107 Tiempo:38s \tTraining Loss: 0.123435 \tValidation Loss: 0.170918\n",
      "Epoch: 108 Tiempo:37s \tTraining Loss: 0.120511 \tValidation Loss: 0.166678\n",
      "Epoch: 109 Tiempo:38s \tTraining Loss: 0.121017 \tValidation Loss: 0.163408\n",
      "Validation loss decreased. Train loss: 0.121017 Validation Loss: (0.165957 --> 0.163408).  Saving model ...\n",
      "Epoch: 110 Tiempo:38s \tTraining Loss: 0.123599 \tValidation Loss: 0.158235\n",
      "Validation loss decreased. Train loss: 0.123599 Validation Loss: (0.163408 --> 0.158235).  Saving model ...\n",
      "Epoch: 111 Tiempo:38s \tTraining Loss: 0.120350 \tValidation Loss: 0.170933\n",
      "Epoch: 112 Tiempo:38s \tTraining Loss: 0.122445 \tValidation Loss: 0.165392\n",
      "Epoch: 113 Tiempo:38s \tTraining Loss: 0.122115 \tValidation Loss: 0.176331\n",
      "Epoch: 114 Tiempo:38s \tTraining Loss: 0.118516 \tValidation Loss: 0.165405\n",
      "Epoch: 115 Tiempo:37s \tTraining Loss: 0.119660 \tValidation Loss: 0.171645\n",
      "Epoch: 116 Tiempo:38s \tTraining Loss: 0.120939 \tValidation Loss: 0.165115\n",
      "Epoch: 117 Tiempo:38s \tTraining Loss: 0.119294 \tValidation Loss: 0.175514\n",
      "Epoch: 118 Tiempo:38s \tTraining Loss: 0.118424 \tValidation Loss: 0.157277\n",
      "Validation loss decreased. Train loss: 0.118424 Validation Loss: (0.158235 --> 0.157277).  Saving model ...\n",
      "Epoch: 119 Tiempo:38s \tTraining Loss: 0.120785 \tValidation Loss: 0.164079\n",
      "Epoch: 120 Tiempo:38s \tTraining Loss: 0.120072 \tValidation Loss: 0.171262\n",
      "Epoch: 121 Tiempo:38s \tTraining Loss: 0.120675 \tValidation Loss: 0.166440\n",
      "Epoch: 122 Tiempo:38s \tTraining Loss: 0.117512 \tValidation Loss: 0.165196\n",
      "Epoch: 123 Tiempo:38s \tTraining Loss: 0.119140 \tValidation Loss: 0.172564\n",
      "Epoch: 124 Tiempo:38s \tTraining Loss: 0.117071 \tValidation Loss: 0.177173\n",
      "Epoch: 125 Tiempo:38s \tTraining Loss: 0.117209 \tValidation Loss: 0.168886\n",
      "Epoch: 126 Tiempo:38s \tTraining Loss: 0.122235 \tValidation Loss: 0.168483\n",
      "Epoch: 127 Tiempo:38s \tTraining Loss: 0.116843 \tValidation Loss: 0.176349\n",
      "Epoch: 128 Tiempo:38s \tTraining Loss: 0.115172 \tValidation Loss: 0.170956\n",
      "Epoch: 129 Tiempo:38s \tTraining Loss: 0.117938 \tValidation Loss: 0.163545\n",
      "Epoch: 130 Tiempo:38s \tTraining Loss: 0.117183 \tValidation Loss: 0.165229\n",
      "Epoch: 131 Tiempo:38s \tTraining Loss: 0.113614 \tValidation Loss: 0.168383\n",
      "Epoch: 132 Tiempo:38s \tTraining Loss: 0.119238 \tValidation Loss: 0.169260\n",
      "Epoch: 133 Tiempo:38s \tTraining Loss: 0.118156 \tValidation Loss: 0.165283\n",
      "Epoch: 134 Tiempo:38s \tTraining Loss: 0.117451 \tValidation Loss: 0.165881\n",
      "Epoch: 135 Tiempo:38s \tTraining Loss: 0.118508 \tValidation Loss: 0.160516\n",
      "Epoch: 136 Tiempo:37s \tTraining Loss: 0.117541 \tValidation Loss: 0.172136\n",
      "Epoch: 137 Tiempo:37s \tTraining Loss: 0.113658 \tValidation Loss: 0.174136\n",
      "Epoch: 138 Tiempo:38s \tTraining Loss: 0.113437 \tValidation Loss: 0.172386\n",
      "Epoch: 139 Tiempo:37s \tTraining Loss: 0.113409 \tValidation Loss: 0.161706\n",
      "Epoch: 140 Tiempo:38s \tTraining Loss: 0.117483 \tValidation Loss: 0.163411\n",
      "Epoch: 141 Tiempo:37s \tTraining Loss: 0.114132 \tValidation Loss: 0.160387\n",
      "Epoch: 142 Tiempo:38s \tTraining Loss: 0.112426 \tValidation Loss: 0.159884\n",
      "Epoch: 143 Tiempo:38s \tTraining Loss: 0.116713 \tValidation Loss: 0.166886\n",
      "Epoch: 144 Tiempo:38s \tTraining Loss: 0.115647 \tValidation Loss: 0.171087\n",
      "Epoch: 145 Tiempo:38s \tTraining Loss: 0.116467 \tValidation Loss: 0.164298\n",
      "Epoch: 146 Tiempo:37s \tTraining Loss: 0.116019 \tValidation Loss: 0.174229\n",
      "Epoch: 147 Tiempo:38s \tTraining Loss: 0.113992 \tValidation Loss: 0.165960\n",
      "Epoch: 148 Tiempo:38s \tTraining Loss: 0.112666 \tValidation Loss: 0.159894\n",
      "Epoch: 149 Tiempo:38s \tTraining Loss: 0.111751 \tValidation Loss: 0.164383\n",
      "Epoch: 150 Tiempo:37s \tTraining Loss: 0.112548 \tValidation Loss: 0.172309\n",
      "Epoch: 151 Tiempo:37s \tTraining Loss: 0.116489 \tValidation Loss: 0.152362\n",
      "Validation loss decreased. Train loss: 0.116489 Validation Loss: (0.157277 --> 0.152362).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# número de epochs para entrenar el modelo\n",
    "n_epochs = 400\n",
    "\n",
    "# wce, dice\n",
    "LOSS_FUNCTION = 'dice'\n",
    "SAVE_MODEL = True\n",
    "\n",
    "if LOSS_FUNCTION == 'wce':\n",
    "    criterion = WeightedCrossEntropyLoss()\n",
    "\n",
    "exists_best_model = False\n",
    "if os.path.isfile(FILENAME):\n",
    "    checkpoint = torch.load(FILENAME)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    valid_losses = checkpoint['valid_losses']\n",
    "    current_epoch = checkpoint['epochs']\n",
    "    best_model_state_dict = checkpoint['best_model_state_dict']\n",
    "    best_optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    #amp.load_state_dict(checkpoint['amp_state_dict'])\n",
    "    exists_best_model = True\n",
    "else:\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    current_epoch = 0\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "start_training = time.time()\n",
    "for epoch in range(current_epoch+1, current_epoch + n_epochs + 1):\n",
    "    start_epoch = time.time()\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target, correct_cell_count, resized_cell_count in train_loader:\n",
    "        target = target.squeeze(0)\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data = Variable(data).cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        datasize = data.size(0)\n",
    "        del data\n",
    "        if LOSS_FUNCTION == 'dice':\n",
    "            target = target_to_one_hot(target).float()\n",
    "            if train_on_gpu:\n",
    "                target = Variable(target).cuda()\n",
    "            # calculate the batch loss\n",
    "            criterion1 = nn.Softmax(dim=1)\n",
    "            output = output.permute(0,2,3,4,1).contiguous().view(-1,2).float()\n",
    "            loss = simple_dice_loss3D(criterion1(output), target)\n",
    "        elif LOSS_FUNCTION == 'wce':\n",
    "            if train_on_gpu:\n",
    "                target = Variable(target).cuda().long()\n",
    "            loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        #with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        #    scaled_loss.backward()\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item() * datasize\n",
    "        del target\n",
    "        del output\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target, correct_cell_count, resized_cell_count in valid_loader:\n",
    "        target = target.squeeze(0)\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data = Variable(data).cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        datasize = data.size(0)\n",
    "        del data\n",
    "        if LOSS_FUNCTION == 'dice':\n",
    "            target = target_to_one_hot(target).float()\n",
    "            if train_on_gpu:\n",
    "                target = Variable(target).cuda()\n",
    "            # calculate the batch loss\n",
    "            criterion1 = nn.Softmax(dim=1)\n",
    "            output = output.permute(0,2,3,4,1).contiguous().view(-1,2).float()\n",
    "            loss = simple_dice_loss3D(criterion1(output), target)\n",
    "        elif LOSS_FUNCTION == 'wce':\n",
    "            if train_on_gpu:\n",
    "                target = Variable(target).cuda().long()\n",
    "            loss = criterion(output, target)\n",
    "        del target\n",
    "        del output\n",
    "        # update average validation loss\n",
    "        valid_loss += loss.item() * datasize\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    # print training/validation statistics\n",
    "    print('Epoch: {} Tiempo:{:.0f}s \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, time.time()-start_epoch, train_loss, valid_loss))\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        best_optimizer_state_dict = optimizer.state_dict()\n",
    "        print('Validation loss decreased. Train loss: {:.6f} Validation Loss: ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        train_loss,\n",
    "        valid_loss_min,\n",
    "        valid_loss,\n",
    "        ))\n",
    "        valid_loss_min = valid_loss\n",
    "        exists_best_model = True\n",
    "    if exists_best_model:\n",
    "        torch.save({\n",
    "            'epochs': epoch,\n",
    "            'best_model_state_dict': best_model_state_dict,\n",
    "            'best_optimizer_state_dict' : best_optimizer_state_dict,\n",
    "            'model_state_dict' : model.state_dict(),\n",
    "            'optimizer_state_dict' : optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'valid_losses': valid_losses,\n",
    "            'valid_loss_min': valid_loss_min,\n",
    "            #'amp_state_dict': amp.state_dict()\n",
    "        }, FILENAME)\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "plot_epochs(train_losses, valid_losses, MODEL_NAME)\n",
    "metrics(model, test_data, save=True, model_name=MODEL_NAME)\n",
    "\n",
    "print(\"Entrenamiento terminado en {:.2f}m\".format((time.time() - start_training)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}