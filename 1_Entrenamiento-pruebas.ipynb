{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#!{sys.executable} -m pip install torchio\n",
    "#!{sys.executable} -m pip install scikit-image\n",
    "#!{sys.executable} -m pip install kornia\n",
    "#!{sys.executable} -m pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "MODEL_NAME = 'sistema_6_MQ'\n",
    "FILENAME = MODEL_NAME + '.pth'\n",
    "DIM_SIZE_REDUCTION = (1,1,1)\n",
    "MODE = 'boundaries'\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from utils import compare_output, metrics, draw_images, plot_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CellsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Credits instructions: https://torchio.readthedocs.io/#credits\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torchio.transforms as transformsio\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import kornia.augmentation as K\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataset import PATHS\n",
    "\n",
    "train_path = PATHS['MQ_TRAIN']\n",
    "valid_path = PATHS['MQ_VALID']\n",
    "test_path = PATHS['MQ_TEST']\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 1\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([transformsio.ZNormalization()])\n",
    "transform_augmentation = nn.Sequential(\n",
    "    K.RandomDepthicalFlip3D(same_on_batch=True), \n",
    "    K.RandomHorizontalFlip3D(same_on_batch=True), \n",
    "    K.RandomVerticalFlip3D(same_on_batch=True),\n",
    "#    K.RandomRotation3D((0.5, 3, 3), same_on_batch=True)\n",
    ")\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = CellsDataset(train_path, target_mode=MODE, transform = transform, transform_augmentation=transform_augmentation, dim_size_reduction=DIM_SIZE_REDUCTION)\n",
    "valid_data = CellsDataset(valid_path, target_mode=MODE, transform = transform, dim_size_reduction=DIM_SIZE_REDUCTION)\n",
    "test_data = CellsDataset(test_path, target_mode=MODE, transform = transform, dim_size_reduction=DIM_SIZE_REDUCTION)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura U-Net reducida. Pensada para hacer entrenamientos rápidos para hacer pruebas en las que la calidad del resultado no importe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MiniUNet3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura U-Net completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import UNet3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciación del modelo. Si se usa CUDA esperar unos segundos a que el modelo se cargue en GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet3D(1,2)\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizador y definición de funciones de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from apex import amp\n",
    "AMP = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=0.00001)\n",
    "\n",
    "# https://github.com/mcarilli/mixed_precision_references/blob/master/Pytorch_Devcon_2019/devcon_2019_mcarilli_final.pdf\n",
    "if AMP:\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "\n",
    "def target_to_one_hot(target):\n",
    "    temp = torch.reshape(target, (-1,)).long()\n",
    "    target = torch.zeros([torch.numel(temp), 2])\n",
    "    target[torch.arange(torch.numel(temp)),temp] = 1\n",
    "    return target\n",
    "\n",
    "from losses import simple_dice_loss3D, WeightedCrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 23.88 GiB total capacity; 19.44 GiB already allocated; 1.10 GiB free; 22.12 GiB reserved in total by PyTorch)\nException raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7fb9e5d8f1e2 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1e64b (0x7fb9e5fe564b in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1f464 (0x7fb9e5fe6464 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: <unknown function> + 0x1faa1 (0x7fb9e5fe6aa1 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so)\nframe #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x11e (0x7fb997d3890e in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xf33949 (0x7fb996172949 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xf4d777 (0x7fb99618c777 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x10e9c7d (0x7fb9d0f28c7d in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0x10e9f97 (0x7fb9d0f28f97 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7fb9d1033a1a in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0xeb8fbf (0x7fb9960f7fbf in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #11: at::native::cudnn_convolution_transpose_backward_input(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool) + 0xac (0x7fb9960f870c in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #12: <unknown function> + 0xf1f8f5 (0x7fb99615e8f5 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #13: <unknown function> + 0xf4f11a (0x7fb99618e11a in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #14: at::cudnn_convolution_transpose_backward_input(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool) + 0x189 (0x7fb9d10376b9 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #15: at::native::cudnn_convolution_transpose_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x179 (0x7fb9960f75a9 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #16: <unknown function> + 0xf1f80b (0x7fb99615e80b in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #17: <unknown function> + 0xf4f27c (0x7fb99618e27c in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #18: at::cudnn_convolution_transpose_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x220 (0x7fb9d1044c50 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #19: <unknown function> + 0x2f03b08 (0x7fb9d2d42b08 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #20: <unknown function> + 0x2f18d1c (0x7fb9d2d57d1c in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #21: at::cudnn_convolution_transpose_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x220 (0x7fb9d1044c50 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::generated::CudnnConvolutionTransposeBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x28a (0x7fb9d2b8f85a in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #23: <unknown function> + 0x3375bb7 (0x7fb9d31b4bb7 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7fb9d31b0400 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7fb9d31b0fa1 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7fb9d31a9119 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7fb9e6b414ba in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)\nframe #28: <unknown function> + 0xbd66f (0x7fba02c0666f in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #29: <unknown function> + 0x76db (0x7fba084176db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #30: clone + 0x3f (0x7fba0875088f in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b4afed5d44dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mAMP\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 23.88 GiB total capacity; 19.44 GiB already allocated; 1.10 GiB free; 22.12 GiB reserved in total by PyTorch)\nException raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7fb9e5d8f1e2 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1e64b (0x7fb9e5fe564b in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1f464 (0x7fb9e5fe6464 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: <unknown function> + 0x1faa1 (0x7fb9e5fe6aa1 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so)\nframe #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x11e (0x7fb997d3890e in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xf33949 (0x7fb996172949 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xf4d777 (0x7fb99618c777 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x10e9c7d (0x7fb9d0f28c7d in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0x10e9f97 (0x7fb9d0f28f97 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7fb9d1033a1a in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0xeb8fbf (0x7fb9960f7fbf in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #11: at::native::cudnn_convolution_transpose_backward_input(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool) + 0xac (0x7fb9960f870c in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #12: <unknown function> + 0xf1f8f5 (0x7fb99615e8f5 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #13: <unknown function> + 0xf4f11a (0x7fb99618e11a in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #14: at::cudnn_convolution_transpose_backward_input(at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool) + 0x189 (0x7fb9d10376b9 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #15: at::native::cudnn_convolution_transpose_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x179 (0x7fb9960f75a9 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #16: <unknown function> + 0xf1f80b (0x7fb99615e80b in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #17: <unknown function> + 0xf4f27c (0x7fb99618e27c in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #18: at::cudnn_convolution_transpose_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x220 (0x7fb9d1044c50 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #19: <unknown function> + 0x2f03b08 (0x7fb9d2d42b08 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #20: <unknown function> + 0x2f18d1c (0x7fb9d2d57d1c in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #21: at::cudnn_convolution_transpose_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, c10::ArrayRef<long>, long, bool, bool, std::array<bool, 2ul>) + 0x220 (0x7fb9d1044c50 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::generated::CudnnConvolutionTransposeBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x28a (0x7fb9d2b8f85a in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #23: <unknown function> + 0x3375bb7 (0x7fb9d31b4bb7 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7fb9d31b0400 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #25: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7fb9d31b0fa1 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #26: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7fb9d31a9119 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #27: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7fb9e6b414ba in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)\nframe #28: <unknown function> + 0xbd66f (0x7fba02c0666f in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #29: <unknown function> + 0x76db (0x7fba084176db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #30: clone + 0x3f (0x7fba0875088f in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# número de epochs para entrenar el modelo\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "# wce, dice\n",
    "LOSS_FUNCTION = 'dice'\n",
    "SAVE_MODEL = True\n",
    "\n",
    "if LOSS_FUNCTION == 'wce':\n",
    "    criterion = WeightedCrossEntropyLoss()\n",
    "elif LOSS_FUNCTION == 'ce':\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "exists_best_model = False\n",
    "\n",
    "if os.path.isfile(FILENAME):\n",
    "    checkpoint = torch.load(FILENAME)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    valid_losses = checkpoint['valid_losses']\n",
    "    current_epoch = checkpoint['epochs']\n",
    "    best_model_state_dict = checkpoint['best_model_state_dict']\n",
    "    best_optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    if AMP:\n",
    "        amp.load_state_dict(checkpoint['amp_state_dict'])\n",
    "    exists_best_model = True\n",
    "    del checkpoint\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    current_epoch = 0\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "start_training = time.time()\n",
    "for epoch in range(current_epoch+1, current_epoch + n_epochs + 1):   \n",
    "    start_epoch = time.time()\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target, correct_cell_count, resized_cell_count in train_loader:\n",
    "        target = target.squeeze(0)\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data = Variable(data).cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        datasize = data.size(0)\n",
    "        del data\n",
    "        if LOSS_FUNCTION == 'dice':\n",
    "            target = target_to_one_hot(target).float()\n",
    "            if train_on_gpu:\n",
    "                target = Variable(target).cuda()\n",
    "            # calculate the batch loss\n",
    "            criterion1 = nn.Softmax(dim=1)\n",
    "            output = output.permute(0,2,3,4,1).contiguous().view(-1,2).float()\n",
    "            loss = simple_dice_loss3D(criterion1(output), target)\n",
    "        elif LOSS_FUNCTION in {'wce', 'ce'}:\n",
    "            if train_on_gpu:\n",
    "                target = Variable(target).cuda().long()\n",
    "            loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        if AMP:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item() * datasize\n",
    "        del target\n",
    "        del output\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target, correct_cell_count, resized_cell_count in valid_loader:\n",
    "        target = target.squeeze(0)\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data = Variable(data).cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        datasize = data.size(0)\n",
    "        del data\n",
    "        if LOSS_FUNCTION == 'dice':\n",
    "            target = target_to_one_hot(target).float()\n",
    "            if train_on_gpu:\n",
    "                target = Variable(target).cuda()\n",
    "            # calculate the batch loss\n",
    "            criterion1 = nn.Softmax(dim=1)\n",
    "            output = output.permute(0,2,3,4,1).contiguous().view(-1,2).float()\n",
    "            loss = simple_dice_loss3D(criterion1(output), target)\n",
    "        elif LOSS_FUNCTION in {'wce', 'ce'}:\n",
    "            if train_on_gpu:\n",
    "                target = Variable(target).cuda().long()\n",
    "            loss = criterion(output, target)\n",
    "        del target\n",
    "        del output\n",
    "        # update average validation loss\n",
    "        valid_loss += loss.item() * datasize\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    # print training/validation statistics\n",
    "    print('Epoch: {} Tiempo:{:.0f}s \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, time.time()-start_epoch, train_loss, valid_loss))\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        best_optimizer_state_dict = optimizer.state_dict()\n",
    "        print('Validation loss decreased. Train loss: {:.6f} Validation Loss: ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        train_loss,\n",
    "        valid_loss_min,\n",
    "        valid_loss,\n",
    "        ))\n",
    "        valid_loss_min = valid_loss\n",
    "        exists_best_model = True\n",
    "    if exists_best_model:\n",
    "        file_obj = {\n",
    "            'epochs': epoch,\n",
    "            'best_model_state_dict': best_model_state_dict,\n",
    "            'best_optimizer_state_dict' : best_optimizer_state_dict,\n",
    "            'model_state_dict' : model.state_dict(),\n",
    "            'optimizer_state_dict' : optimizer.state_dict(),\n",
    "            'train_losses': train_losses,\n",
    "            'valid_losses': valid_losses,\n",
    "            'valid_loss_min': valid_loss_min,\n",
    "        }\n",
    "        if AMP:\n",
    "            file_obj['amp_state_dict'] = amp.state_dict()\n",
    "        torch.save(file_obj, FILENAME)\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "plot_epochs(train_losses, valid_losses, MODEL_NAME)\n",
    "metrics(model, test_data, save=True, model_name=MODEL_NAME)\n",
    "\n",
    "print(\"Entrenamiento terminado en {:.2f}m\".format((time.time() - start_training)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
